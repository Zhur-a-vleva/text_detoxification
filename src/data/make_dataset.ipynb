{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba72fd1a-9d76-458b-97d1-b40cb30a20de",
   "metadata": {},
   "source": [
    "# Code to make the dataset from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e47b6da-9b80-4caa-9778-19eb02568ac6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b848040-927c-4a43-8db0-077865440958",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Darya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Darya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Darya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663aeb4-1b8f-4e00-bc0b-7a02fa92727b",
   "metadata": {},
   "source": [
    "## Class MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06733e02-7015-41b2-a9f4-81288cb5fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.raw_data = pd.read_csv(data_path, sep='\\t', index_col=0)\n",
    "        \n",
    "        data = pd.DataFrame()\n",
    "        data['toxic'] = pd.concat([self.raw_data[self.raw_data['ref_tox'] > self.raw_data['trn_tox']]['reference'], self.raw_data[self.raw_data['ref_tox'] < self.raw_data['trn_tox']]['translation']])\n",
    "        data['normal'] = pd.concat([self.raw_data[self.raw_data['ref_tox'] > self.raw_data['trn_tox']]['translation'], self.raw_data[self.raw_data['ref_tox'] < self.raw_data['trn_tox']]['reference']])\n",
    "        data['toxic_reduction'] = abs(self.raw_data['ref_tox'] - self.raw_data['trn_tox'])\n",
    "        self.data = data\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.iloc[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95720aa7-59a9-4e3c-9ea7-ed2fd84857df",
   "metadata": {},
   "source": [
    "## Main: prepare tha data, create dataset, transform and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2239d29e-f492-420d-8408-42b34c0a81c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    # remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # replace punctuation with spaces\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "    \n",
    "    # remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # remove non-ascii characters\n",
    "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "    \n",
    "    # remove urls\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # remove html tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "697446be-57a7-4e11-bd8e-1e08c22e312c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    data_dir = \"../../data/raw/filtered_paranmt/\"\n",
    "    dataset_path = \"../../data/interim/text_dataset.pkl\"\n",
    "    filename = \"filtered.tsv\"\n",
    "    \n",
    "    \n",
    "    # create dataset\n",
    "    dataset = MyDataset(os.path.join(data_dir, filename))\n",
    "    \n",
    "\n",
    "    # transform dataset\n",
    "    # clear text\n",
    "    dataset.data.toxic = dataset.data.toxic.apply(clean_text)\n",
    "    dataset.data.normal = dataset.data.normal.apply(clean_text)\n",
    "\n",
    "    # tokenize text\n",
    "    dataset.data.toxic = dataset.data.toxic.apply(word_tokenize)\n",
    "    dataset.data.normal = dataset.data.normal.apply(word_tokenize)\n",
    "\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    dataset.data.toxic = dataset.data.toxic.apply(lambda text: [word for word in text if word not in stop_words])\n",
    "    dataset.data.normal = dataset.data.normal.apply(lambda text: [word for word in text if word not in stop_words])\n",
    "    \n",
    "    # lemmatize text\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    dataset.data.toxic = dataset.data.toxic.apply(lambda text: [lemmatizer.lemmatize(word) for word in text])\n",
    "    dataset.data.normal = dataset.data.normal.apply(lambda text: [lemmatizer.lemmatize(word) for word in text])\n",
    "    \n",
    "    \n",
    "    # save dataset\n",
    "    if not os.path.exists(os.path.dirname(dataset_path)):\n",
    "        os.makedirs(os.path.dirname(dataset_path))\n",
    "    pickle.dump(dataset, open(dataset_path, 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
