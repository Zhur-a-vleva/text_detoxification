{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "163c2e80-4e59-4473-b101-c93a4ff78038",
   "metadata": {},
   "source": [
    "# Script for predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2800fc-8498-4fdb-a81d-0c5dc5a12e55",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0489fb5e-c8e1-46d5-b87f-bd1a88587d67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dafb421-cbb7-4b43-bafd-1a0a4d57247d",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ea1226b-3560-4480-adf8-f632a3bc5447",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you're getting nasty.\n",
      "now get nasty\n",
      "\n",
      "I like that shit.\n",
      "i like\n",
      "\n",
      "You must've been pissed. Damn straight.\n",
      "oh god straight\n",
      "\n",
      "The guy is totally irresponsible.\n",
      "oh god irresponsible\n",
      "\n",
      "Damn it!\n",
      "hell\n",
      "\n",
      "I don't have to do shit.\n",
      "i don't need\n",
      "\n",
      "Trying to kill Ethan.\n",
      "trying kill ethan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    checkpoint = \"t5-small\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    test_model = AutoModelForSeq2SeqLM.from_pretrained(\"../../models/saved/\")\n",
    "    \n",
    "    file = open(\"../../data/raw/translate.txt\", \"r\")\n",
    "    for s in file:\n",
    "        input_ids = tokenizer(s.lower().split(), max_length=128, truncation=True, is_split_into_words=True, return_tensors=\"pt\").input_ids\n",
    "        outputs = test_model.generate(input_ids, max_new_tokens=133)\n",
    "        print(s.strip())\n",
    "        print(tokenizer.decode(outputs[0], skip_special_tokens=True).strip())\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
